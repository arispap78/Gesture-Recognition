\documentclass[12pt]{book}
\usepackage{graphicx}     %For figures
\usepackage{epstopdf}     %<----------
\usepackage{chapterbib}
\usepackage{times}
\usepackage{amsmath,amssymb} %For math
\usepackage{tabularx}
\usepackage[breaklinks]{hyperref} %breaklinks option enabled, so that toc page numbers won't move out of their column, may break links / [colorlinks] to color links
\usepackage{epsfig}       %For eps figures
\usepackage{hyphenat}     %To add manual hyphenation editing capability
\usepackage{comment}      %To add comment environment
\usepackage{bm}           %For bold stuff
\usepackage{subfigure}    %For subfigures
\usepackage{array}        %For better control of tabulars
\usepackage{tabu}         %For better control of tabulars
\usepackage{fixltx2e}     %Fixes some issues of latex2e
\usepackage{indentfirst}  %To indent the first paragraph of every section
%\usepackage{float}       %Can force an image to be placed at a specific place if H is used.
\usepackage{multirow}
\usepackage{url}
\usepackage[T1]{fontenc}
\usepackage{enumerate}

\begin{document}
\title{
{GESTURE RECOGNITION} \\
{\large Aristotle University of Thessaloniki}\\
{\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/banner-horizontal-black72ppi.png}
  \\
  \label{fig:fig0}
\end{figure}}
}
\author{Papanastasiou Nektarios, Prof. Ioannis Pitas}
\maketitle
\date

\setcounter{page}{1}

\chapter*{Summary}
In this article a presentation attempt is made for the gesture recognition. In the beginning there is an explaination about what gestures are and what are the different types of them. The sensors which are used for capturing gestures and create modalities of dataset are also discussed. The applications of the gesture recognition and the interaction between a human and a machine or a robot or a UAV is analysed and some of the latest and most important datasets for gesture recognition are later presented. Moreover, the algorithms that have been developed and the problems that have arisen over the gesture recognition are examined. There is a presentation of the structure of the 3D Convolutional Neural Networks and their layers and in addition to them the combination with the Long Short Term Memory models. Lastly, systems for spatiotemporal analysis of data are studied. CNN models for gesture recognition are compared and a proposed skeleton-based architecture is presented. The article has references from 16 scientific papers of Computer Vision and their Open Access versions, provided by the Computer Vision Foundation (CVF) and from 5 articles on websites on the internet.

\tableofcontents

\chapter*{Introduction}\label{intro}
Gesture recognition is the interpretation of any gesture from a computer through mathematical algorithms so it can execute commands based on those gestures. The user performs gestures to control or interact with a machine without using any physical touch.  It is a form of interaction between a human and a computer, a robot or a device and nowadays it is applied more often and it is widespread as a state of art technology. Computer vision is the scientific field that deals with the creation of algorithms for the interpretation of images and videos which are recorded by sensors, thus gesture recognition is one of its objects of study. In this article, an attempt will be made to analyze in detail the way in which gestures are recognized with computer vision algorithms. 

\chapter{Gesture Types}\label{ch:ch1}
Gestures can be extensive and comprehensive or small and contained and they can include any type of movement. Different types of gestures will be mentioned below categorized in terms of time, body parts and duration: 

\section{Gesture types (based on the timing)}\label{s:sec1}
\begin{itemize}
    \item Online gestures: Instant interpretation of gestures. They are used to    manipulate an object (scaling, rotation). 
    \item Offline gestures: they are implemented after the procedure of the user interaction with the object. 
\end{itemize}
    
\section{Gesture types (based on the body parts)}\label{s:sec2}
\begin{itemize}
    \item Hand Gestures: waving goodbye, showing points… 
    \item Head Gestures: nodding, winking… 
    \item Body Gestures: kicking, raise knee or elbow… 
\end{itemize}    

\section{Gesture types (based on duration)}\label{s:sec3}
\begin{itemize}
    \item Static Gesture: the pose in an instant time, a photo or an image of the position of the body part (showing the thumb…).
    \item Dynamic Gesture: changeable pose over a small period of time (waving palm…). 
\end{itemize}

\chapter{Evolution of Gesture Evolution Devices}\label{ch:ch2}
\section{Data Gloves}\label{s:sec4}
Those gloves calculate the position and the movement. They map every movement of phalange and wrist joints accurately with the sensors they carry. The advantage is that in this way, no data processing step for obtaining descriptors is needed, as in the case of the images from a camera. On the other hand, the disadvantage is that data gloves are expensive and not so comfortable for a user. In 1983, it was the first time that a data glove recognized hand position. There are two types of data gloves:

\begin{itemize}
    \item The active gloves have a sensor or accelerometer and they are connected to the computer by cables or with wireless technology. 
    \item The passive or non-invasive gloves have colour markers for image identification (\ref{fig:fig1}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image1.jpg}
  \\
  \caption{Passive gloves to help differentiate finger position \cite{JES2020} .}
  \label{fig:fig1}
\end{figure}

\section{EMG-Electromyography Electrodes}\label{s:sec5}
Instead of data gloves, there are wearable bracelets with electromyography sensors which measure the electrical signals from the muscles. In this way there are no environmental interferences like in the voice interaction or in computer vision systems. Electromyography is based on the study of the neuromuscular system, so with EMG we can detect, analyze and process the electrical signals which are produced by muscles and nerves, by using electrodes. EMG has been used for medical diagnosis, control of prosthetics and for the rehabilitation after severe musculoskeletal injuries.

\section{Ultrasound}\label{s:sec6}
There are two techniques for gesture capturing by ultrasound: 

\begin{itemize}
    \item sonomyography: uses ultrasound images and provide the observation of the muscles in the human body on a real-time. in contrast with EMG, sonomyography does not have difficulty of differentiating between individual muscles (for non-invasive techniques), and those lying deeper in the forearm so it captures the variety of wide hand movements.
    \item A technique based on Doppler Effect uses ultrasonic frequency signals where a device emits ultrasonic continuous tones. This can be done thanks to the fact that our tissues have different acoustic impedances, so when a sound wave passes from one to another with a different acoustic impedance, different amounts of energy are reflected, thus forming an ultrasound image. 
\end{itemize}

\section{WiFi}\label{s:sec7}
WiFi has the ability to perform (NLOS) non line of sight gesture recognition. There are already researches for the strength of a signal indicator-RSSI, also on the signal of the indicator of flight time –ToF, or where there is an observation of the channel status information-CSI. However, these kinds of systems require either specialized devices, or it can be a modification by already-existing commercial devices, and some of them are excessively susceptible to interference. The most important advantage of this technique is that it is able to capture gestures for providing gesture recognition even without direct vision (NLOS) (\ref{fig:fig2})

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image2.jpeg}
  \\
  \caption{WiFi recognition of different hand positions \cite{JES2020} .}
  \label{fig:fig2}
\end{figure}

\section{RFID-Radio Frequency Identification}\label{s:sec8}
RFID systems embody ultra-high frequency-UHF readers from the commercial market, which they can detect labels. Information such as phase changes, which are received from UHF signals and they can be used for gesture recognition with very good results. Identification tags can be carried by a person or without anyone, based on the type of RFID application, and they can work even without a battery (active and passive). RFID tags are usually used for supply chain management and automatic object identification. The advantage of the use of passive RFID technology is the low cost, but on the other hand, since they do not count with their own energy supply, the detection ranges are reduced to centimeters.

\section{RGB Cameras}\label{s:sec9}
The most widely used devices for gesture captures are definitely RGB cameras. Digital cameras receive the light and its three channels, the red, the green and the blue light. After that  record the image as an array of the pixels, which they have all the three RGB channels. The object will be detected from the image with an algorithm of computer-vision, in which, the color’s intensity and order of every pixel of the image will give information.


\section{Depth Camera}\label{s:sec10}
This camera gives depth data which provides a map for the depth in each pixel. The depth camera has a fourth channel for the depth information, the distance between the focus and the captured object. In this way, the depth map describes the distance of an object. The advantage of this data is that it gives images of the segmentation of a body part as it tracks the skeleton. In this way, with the  track and the segment of the hands, depth data can be used specifically for the illustration of the hands. On the other hand, there can be many distortions because of the flaws of the small lense.
(\ref{fig:fig3},\ref{fig:fig4}, \ref{fig:fig5}).

\begin{figure}[!htbp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=0.55\textwidth]{figures/image3.jpg}
	\\
	\caption{Microsoft Kinect depth Camera.}
	\label{fig:fig3}
\end{figure} 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image4.png}
  \\
  \caption{The Kinect capture image using infrared radiation}
  \label{fig:fig4}
\end{figure}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image5.png}
  \\
  \caption{Image is processed with the use of color gradients (white-near objects to blue-far objects) into a depth map}
  \label{fig:fig5}
\end{figure}

\section{Leap Motion}\label{s:sec11}
This device is compact and economical and it is used for recognition. It can track the 3D forearms,  the  hands and the fingers in real time. It consists of two infrared cameras, situated in an 120° angle and three LEDs on  infrared radiation. The sensors are made to operate in the non-visible spectrum for eyes of the human, and the speeds that it use are of up to 200 fps-frames per second, and they adapt their lighting so they are able to detect light. In addition, sensors are CMOS type, which means that no external electronics are required, because the digitization of each pixel occurs within each cell. Because of this, the capture speed is faster, and there is less use of the hardware space (\ref{fig:fig6}).

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image6.png}
  \\
  \caption{Leap Motion operating mode \cite{JES2020}}
  \label{fig:fig6}
\end{figure}

\section{VR-Virtual Reality}\label{s:sec12}
The VR devices can be used indifferent fields and build interactive spaces, one of them is to track fingers. There have been a lot of efforts on creating spatial awareness, close to real and more accurate, to create a full user experience. Therefore, users want to achieve the head-mounted display (HMD), a mounted screen that leads to the highest possible degree of presence. After the tracking of the movements of the user, they are transformed into capricious coordinate systems and data formats, and in this way the VR environment that is created provides as a meta-layer for integration and simulation of different virtual tracking systems.

\chapter{Applications}\label{ch:ch3}
The gesture recognition is applied in many different fields. Some of the most common application areas of it are: 

\begin{itemize}
    \item Sign Language is one of the most important areas that gesture recognition can be very useful.
    \item The navigation or/and the manipulation in VR environment is another promising area that gesture recognition will play a basic role. 
    \item In Distance learning also can be used.
    \item The Understanding of the human behavior in the interaction of a human with a computer is the field that now we can see the first results. 
    \item Lastly, The distance controlling of devices and of machines is one of the subjects of the computer vision science.
\end{itemize}

\section{Human-Machine Interaction}\label{s:sec13}
The interaction between a human and a device can be performed with gesture recognition. The paper \cite{ZEN2018},presents a driver–vehicle interaction and the sensor that is used for the getting the depth data is the Camboard Nano one of the types of Time-of-Flight-ToF sensors. The control system is applied on a computer-tablet which is situated on the center of the console of the vehicle. ToF sensors are able to record as depth data the hand gestures in real time.Camboard nano sensor provides depth images, their resolution is 165 × 120 pixel at a frame  rate of 90 fps. The extracted three-dimensional point clouds are being preprocessed and serve as input for machine learning methods (\ref{fig:fig7}).

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image7.jpeg}
  \\
  \caption{Performing hand gesture detected in the range of the sensor of time-of-flight-ToF (area of detection in red)\cite{ZEN2018}}
  \label{fig:fig7}
\end{figure}

\subsection{Preprocessing data}\label{subsec:ssec1}
Depending on the number of hand gesture recognitions and the dimensionality of the input data, there must be robust preprocessing methods for the classification system, thorough hyperparameter tuning to achieve high recognition rates in a low runtime. The data carries irrelevant information for the hand gesture classification.PCA (Principal Components Analysis) cuts down the dimensions of data with many dimensions and it is used for cropping the data to the part we want to focus on.(\ref{fig:fig8}). The equation of the eigenvector for a covariance matrix of the depth images calculates the direction of largest variance. For an n three-dimensional coordinates input vector x, the computation of the mean value m: $m=\frac 1 n\sum_{i=1}^n x_i$.
The estimation of  the covariance matrix as scatter matrix C: $C=\sum_{i=1}^n (x_i-m)\dot(x_i-m)^T$.

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image10.jpeg}
  \\
  \caption{Preprocessing data with PCA the green area (left) remains to identify the correct hand posture class (right) 
 \cite{ZEN2018}}
  \label{fig:fig8}
\end{figure}

In an example of testing the system, a participant drives a simulated vehicle on a predefined course with three lanes at a fixed velocity, and performs a mid-air hand gesture. With the gesture recognition the system interprets the command to change the lane (\ref{fig:fig9}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image11.jpeg}
  \\
  \caption{Performing the test of the Lane Change with the control of the system with gestures \cite{ZEN2018}}
  \label{fig:fig9}
\end{figure}

\section{Human-Robot Interaction}
Gesture controlled robots are used in various fields: 

\begin{itemize}
    \item In industry (car factories…).
    \item In medicine (remote surgeries with robots…). 
    \item In military (armed robotic vehicles…).
    \item In space (articulated hands…).
\end{itemize}

\subsection{Robot Programming Using Gestures}
In the paper \cite{TSA2016} proposed a method to program a robot using body and hand gestures. The method is integrated within an open communication architecture based on Robot Operating System (ROS), so in this way it can be extended with new functionalities. The data formed from two devices. The first device is an RGB-d for the body gestures, and the second is leap motion for the hand gestures. There is a vocabulary of body and hand gestures, allowing the movement of robot in different directions. The body gestures monitor the motion of the robot arm and they do it in six directions +x,-x, +y,-y, +z, -z. (\ref{fig:fig10}). The hand gestures are made in dynamic way and they involve movements of the fingers and they create the identical movements as the gestures of the body. The system is applied to an industrial robot for the interaction with the user which is being executed on line, and it can perform in every robot, with the execution of the coordinated movements.

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image12.jpg}
  \\
  \caption{Performing body gestures as commands for robot \cite{TSA2016}}
  \label{fig:fig10}
\end{figure}

\subsection{The Structure of the System}
The two sensors (Kinect and leap motion) are connected to computer. The computer and the robot create a network through an ethernet cable. A vocabulary includes body and hand gestures as high-level commands for the robot. The body gestures are interpreted by the Kinect and the model recognizes 18 human skeleton nodes. Analysis of the nodes ends up to the classification which is based to the vocabulary. The data from the leap motion sensor is used to recognize of the hand gestures in the same way. (\ref{fig:fig11}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image13.jpg}
  \\
  \caption{The architecture of the hardware \cite{TSA2016}}
  \label{fig:fig11}
\end{figure}

\section{UNMANNED AERIAL VEHICLE}
UAV-unmanned aerial vehicle is used in many different purposes: 

\begin{itemize}
    \item In photography (aerial photos…).
    \item In military (surveillance…).
    \item In transportation (UAV taxi…).
    \item In delivery (courier services…).
    \item In agriculture (spray for pest and fungal infestations…). 
\end{itemize}

\subsection{Payload of UAVs}
Besides its weight, a UAV can carry additional material such as sensors, cameras, packages for deliver etc. Depending on the size and the power of the UAV, different types of sensors can be implemented \cite{AZO}: 

\begin{itemize}
    \item Thermal Sensors: detect the heat that radiates from any observed object and create data with images or videos. 

\item Chemical Sensors: detect the chemicals of the environment. The main use of them is for the safety of the workers or generally people in unknown chemical present 

\item Time of Flight (ToF) Sensors: Depth ToF sensors send out a short light pulse in the infrared radiation and they measure the time this infrared light pulse needs to return to the drone.

\item Distance Sensors: 

\item Light-Pulse Distance Sensing (Laser): A LASER RANGE FINDER-LRF emits a beam of laser, detects the time that the beam returns after reflecting an object and calculates the distance.

\item Radio Detection and Ranging: detect the speed and direction of the object in relation to the drone depending on their frequency shift. 

\item Sonar-Pulse Distance Sensing (Ultrasonic): send a sound wave at a specific frequency to an object and detect the time that it returns. 

\item Orientation Sensors: 

\item Accelerometers: perceive movement of the drone. 

\item Inertial Measurement Sensors: detect changes of direction 

\item Some of the most used cameras on drones are: 

\item RGB camera 

\item RGB-depth camera 

\item Leap motion camera 

\item NIR(Near-Infrared) camera 

\item SWIR (short-wave infrared) camera 

\item LIDAR (Light Detection And Ranging) camera 
\end{itemize}

\section{Human-Drone Interaction}
The paper \cite{HUA2019} proposed a model that performs a remote control of drones by gesture recognition. The structure of the model has 5 modules (\ref{fig:fig12}):

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image14.png}
  \\
  \caption{The structure of the model \cite{HUA2019}}
  \label{fig:fig12}
\end{figure}

\begin{itemize}
    \item The scene-understanding module plays an important role, so the UAV can be able to execute in different environments and working conditions. 

\item The pilot detection module detects and isolates the real UAV pilot from any other person, extracts the skeleton points from the pilot, and determines the correct hand regions for gesture recognition. 

\item The action detection and recognition module recognize human actions by using the key points extracted from the pilot in the pilot detection module, as the input features (\ref{fig:fig13}). The description of human motion can be recognized with the use of the key points which are derived as features from the bodies that are the input of the GCN-Graph Convolutional Network. $$f_{out}^t(x_c)=\sum_{h=1}^K\sum_{w=1}^K f_{in}(S(x_c),h,w)\dot W(h,w)$$
S is the function of the samples which is utilized to itemize the location neighbors x.W is the function of the weights and it gives a vector of weights to compute the inner product with the input of the samples feature vectors within kernel of the convolution. 
, H and w are the pixels in the kernel (rows and columns). There are 6 kinds of actions for the control the action of UAV: $a_i$ for $i=1 \dots 6$.
The computation of the probability of an input action $a^{in}$ to be classified as action $a_i$ with a softmax function $P(a_i|a^{in})$ through M successive frames is: $$P(a_i|a^{in})=softmax(\sum_{t=1}^M\sum_{c=1}^N) f_{out}^t(x_c))$$ If this function has a maximum output value over threshold which is predefined, then we can consider that the input action is the corresponding action.
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image21.png}
  \\
  \caption{a skeleton sequence  represented with the ST-GCN spatial temporal graph, which is used for the classification of the action of the user \cite{HUA2019}}
  \label{fig:fig13}
\end{figure}

\begin{itemize}
    \item The gesture recognition module extracts the personal features of each pilot who will control the UAV and sorts them in his personal feature library before the entire system is operating. That helps the model to personalize the gestures and to avoid false interpretations. 

\item The joint reason and control module is formed to segregate similar gestures. For example, the actions of hands up and that of drawing circle are almost the same (\ref{fig:fig14}). 
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image22.jpeg}
  \\
  \caption{Two users perform identical gestures and the result of the system \cite{HUA2019}}
  \label{fig:fig14}
\end{figure}

\chapter{Datasets for Gesture Recognition}
Recently, deep learning models are the ultramodern methodology of gesture recognition. Data inputs of various modalities(the skeleton joints, the shape of the body of human, RGB, the optical flow, and the depth frames) are combined for the training of these models, therefore there is a significant amount of datasets which had been created based on the recognition of the gesture.

\section{DVS128 Gesture Dataset [DVS]}
The paper of CVPR 2017 “A Low Power, Fully Event-Based Gesture Recognition System.” describes a real time model of gesture Recognition which used this dataset for training. The dataset comprises 1,342 instances of a set of 11 hand and arm gestures, grouped in 122 trials collected from 29 subjects under 3 different lighting conditions. Each trial consists of 2 files: the data file which contains the events of DVS128, and the annotation file  which describes the times of the beginning and of the end for each gesture. Filenames describe the condition of the illustration and the user in each trial. The event-based gesture recognition system in which the data was recorded used two devices: a DVS128 camera to generate input events, and a TrueNorth processor to inspect the input event stream. The DVS128 camera is a 128 × 128-pixel Dynamic Vision Sensor that generates events only when a pixel value changes magnitude by a user-tunable threshold. The IBM TrueNorth chip is a reconfigurable, non-von Neumann processor containing 1 million spiking neurons and 256 million synapses distributed across 4096 parallel, event-driven, neurosynaptic cores.

\section{The use of 3D Skeletal Dataset for Hand Gesture Recognition [SKD]}
There are 14 hand gestures in this dataset executed in 2 ways: with a specific finger and with the whole hand. There are 2800 sequences, every gesture is performed in a range of 1 to 10 times by 28 subjects in both of the ways. All participants are right-handed. In every sequence are labelled the gesture, the number of the used fingers, the user and the trial. In the 2D depth image space and in the 3D world space there are 22 joints which form a full hand skeleton and their coordinates are contained in each frame of sequences. The skeletons were recorded at 30 frames per second. The training set has 1960 gesture sequences (70\% of the dataset) of maximum size of 171 frames and the testing set has 840 gesture sequences (30\% of the dataset) of maximum size of 162 frames. 

\section{HGM-4}
The HGM-4 dataset \cite{GM4} is built for hand gesture recognition and contains 4,160 color images (1280×700 pixels) of 26 hand gestures which are recorded by 4 cameras, each one at a different position. The images were taken indoor in various positions and the background was removed semi-automatically. This dataset can be used for studying the hand-gesture recognition problems in the perception of multiple views. Every image from 4 cameras were combined to be in the training set or in the testing set with all possible combination.

\section{EgoGesture Dataset}
The dataset consists of 2,081 RGB-D videos, 24,161 samples of gestures and 2,953,224 frames from 50 participants.\cite{EGO2018}. There are static or dynamic gestures classified in 83 classes and they are performed with wearable devices. The videos was made in 6 different scenes, 4 indoor and 2 outdoor. There are videos where the gestures were performed by people while they were walking. The two-modality videos, RGB and depth modules, are recorded in a resolution of 640x480 pixel with 30 fps. The gestures performed in random, thus the videos can be applied for the evaluation of the gesture detection in sequence. The volume of the data is about 46G of RGB-D videos and about 32G of images(320x240) (\ref{fig:fig15}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image23.png}
  \\
  \caption{The 83 classes of the EgoGesture dataset \cite{EGO2018}}
  \label{fig:fig15}
\end{figure}

\section{putEMG AND putEMG-Force}
These are datasets of electromyographic activity which was captured from the surface of the forearm. \cite{PEM}. The dataset includes 44 healthy subjects (8 females, 36 males) aged 19 to 37 years old. Each subject participated in the experiment twice, with a minimum one-week interval, performing same procedure. Signals were captured from 24 electrodes which were fixed around participant right forearm with the use of 3 elastic bands, with the creation of a 3×8 matrix, data is sampled at the rate of 200 Hz. The dataset contains 7 active gestures (hand flexion, extension…) + idle and a set of trials with isometric contractions.
They are used for gesture recognition and for grasp force recognition. While a gesture was executed, a HD Camera giving an RGB feed and a depth camera with a close view of hand of the participant were utilized. Depth images and videos of the performing  of the gestures are equipped with the EMG data. (\ref{fig:fig16}).

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image24.png}
  \\
  \caption{Signals were captured from 24 electrodes which were fixed around participant right forearm: elbow band [1-8], middle band [9-16], wrist band [17-24] \cite{PEM}}
  \label{fig:fig16}
\end{figure}

\section{HMD Gesture Dataset}
This dataset contains about 360,000 image pairs for gesture recognition and were taken from 31 participants and 30 different environments \cite{HMG}. The images hand gestures for AR and VR head-mounted display systems and they are recorded from a stereo monochrome fisheye pair mounted in front of an HMD system. The dataset contains 8 gesture classes in 4 pairs: \emph{left hand- right hand, left peace- right peace, left thumbs up,-right thumbs up, left thumbs down-right thumbs down}. For each image pair, , it is provided a label  of a gesture class and a bounding box where hands are located. Images also may contain cluttered background and exacting lighting conditions. 

\section{WLASL}
The dataset, WLASL, is a large-scale signer-independent ASL dataset, 34.404 videos of 3.126 glosses \cite{DON2020}. The meta data has annotations about: 

\begin{itemize}
    \item Temporal boundary: the indication of the start and end frames of a sign. 

    \item Body Bounding-box: the reduction of the noises with the use of YOLOv3 for person detection, to identify the body of the signers in videos. 
    
    \item Signer Diversity: there are inter-signer variations in the input data (example: signer appearance and signing paces). 
    
    \item Dialect Variation Annotation: the dialect variations of signs contain different sign primitives, such as hand-shapes and motions, have been annotated for each gloss. 
\end{itemize}

The videos have lengths ranging from 0,36 to 8,12 seconds, and the average length of all the videos is 2,41 seconds. The average intra-class standard deviation of the videos is 0,85 seconds.

\section{The 20BN-jester Dataset V1}
The video data is split into parts of 1 GB and the number of the videos is 148.092 and their total size is 22.8 GB \cite{JES}. Total number of frames is 5.331.312. The videos for the training set are 118.562, for the testing set 14.743, and for the validation set 14.787. The 1.376 actors have recorded a set of 27 actions. The average duration of the videos is 3 second and the average number of videos for every participant is 43. The JPG images from the videos were extracted at 12 frames per seconds and there are 27 classes of the dataset. The subjects performed hand gestures in front of a laptop camera or webcam. The dataset focuses on a small set of action categories that encompass the most commonly performed human gestures in the context of visual human-computer interfaces.

\section{UAV-Gesture}
The data \cite{ASA2018} was collected on a place located in a wheat field from a rotorcraft UAV (3DR Solo) in low-altitude and slow flight. For video recording, a GoPro Hero 4 Black camera was used with an anti-fish eye replacement lens. The videos have a resolution HD (1920 × 1080) at 25 fps. In the videos, the participant is located in the center of the frame and execute 13 gestures, each of the gesture is performed in a range of five to ten times. When recording the gestures, sometimes the UAV drifts from its initial hovering position due to wind gusts. This adds random camera motion to the videos making them closer to practical scenarios. There are rich variations in the recorded gestures in terms of the phase, orientation, camera movement and the body shape of the actors. There is an annotation  of 13 body joints in 37151 frames, the ankles, the knees, the hip-joint, the wrists, the elbows, the shoulders and the head and each annotation has also the gesture class, the participant identity and the bounding box. With the addition of a margin to the maximum and minimum coordinates of joint annotations in x and y directions, there is the creation of the bounding box.

\chapter{Algorithms of Gesture Recognition}
There is a variety of different algorithms for gesture recognition (\ref{fig:fig17}):
\begin{itemize}
    \item 3D model-based algorithms: the interpretation of an object as a list of vertices and lines in the 3D mesh version.
    \item Skeletal-based algorithms: models the object but with less parameters than the version of volumetric. 
    \item Appearance-based models: acquire the parameters from the images or from the videos directly, with the use of a template database.
    \item Electromyography-based models:  classify the body movement with data of electrical signals generated by the muscles.
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image25.png}
  \\
  \caption{The tree of algorithms}
  \label{fig:fig17}
\end{figure}

The differences of the algorithms of the tree in the image above and how they represent a hand model each one of them are shown below (\ref{fig:fig18}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image26.jpg}
  \\
  \caption{different models of hand methods to illustrate hand posture (a) 3D volumetric model. (b) 3D geometric model. (c) 3D Skeleton model. (d) Colored marker-based model. (e) Non-geometric shape model (Binary silhouette). (f) 2D deformable template model \cite{RAF2012}}
  \label{fig:fig18}
\end{figure}

\chapter{Problems of Gesture Recognition}
\begin{itemize}
    \item Lighting condition: the changes of the light in the image make the extraction of the skin region more difficult.  

\item Rotation: there is problem when the subject that performs the gesture is rotated in any direction. 

\item Scaling: the pose of the body or the hand have different sizes in the image of the gesture. 

\item Interpretation: the changes of the position of the subject in different images, may give false representation of the features.  

\item Background: when there is complex background with different objects that confuse the detection and lead to misclassification. 
\end{itemize}

\chapter{3D CNN-Convolutional Neural Networks}
3D CNN architectures can be applied in a batch of  frames as inputs thus they are used for video analysis. In recent years, the CNN approach has been revived owing to the huge advancements on computational hardware such as the general-purpose graphics processing units (GPUs). The CNN differs from the classical FC-NNs by its weights sharing mechanism. 2D CNNs analyze the spatial dependencies and 3D CNNs analyze both the spatial and temporal relations of the frames. Real-time systems for recognition demand the detection of the gesture and classify it on the stream of images continuously.Many layers are the same as in the 2D CNN but all the calculations are in the 3 dimensions. Rectified linear unit (ReLU) is one of the most common activation functions, avoiding in this way the problems of the vanishing and exploding gradient descent. The 3D-CNN takes the preprocessed phase voxels as the input. Subsequent multiple convolutional layers serve as the critical composition of the CNN with 3D convolution filters and pooling operation. The way that a convolution and a pooling is computing in the input data of a 3D model is shown below (\ref{fig:fig19}, \ref{fig:fig20}, \ref{fig:fig21}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image27.png}
  \\
  \caption{Convolution in a 3D CNN architecture \cite{RAO2020}}
  \label{fig:fig19}
\end{figure}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image28.png}
  \\
  \caption{Before max pooling in 3 dimensions \cite{RAO2020}}
  \label{fig:fig20}
\end{figure}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image29.png}
  \\
  \caption{After max pooling in 3 dimensions \cite{RAO2020}}
  \label{fig:fig21}
\end{figure}

\section{Activation Functions}
\begin{itemize}
    \item ReLU: $f(x)=max(0,x)$
    \item Sigmoid function: $f(x)=\frac 1 {1+e^{-x}}$
    \item Tanh function: $f(x)=tanh(x)=\frac {e^x-e^{-x}} {e^x+e^{-x}}$
\end{itemize}

\subsection{ReLU}
The value of the output $gamma$ at position (x, y, z) on j-th feature map in i-th 3D convolutional layer has a mathematical expression:
 $${\gamma}_{j,xyz}^i=ReLU(b_j^i+\sum_{m=1}^{M^{i-1}}\sum_{p=0}^{P^i-1}\sum_{q=0}^{Q^i-1}\sum_{r=0}^{R^i-1} {w_{jm,pqr}^i \cdot {\gamma}_{m,(x+p)(y+q)(z+r)}^{i-1}})$$

$b_j^i$ is the common bias for j-th feature map. $w_{jm,pqr}^i$ is the (p, q, r)-th value of the 3D filter for j-th feature map at i-th layer associated with the m-th feature map in the (i-1)-th layer. $M^{i-1}$ is the number of feature maps at (i-1)-th layer. $P^i$, $Q^i$ and $R^i$ denotes the size of the 3D filter at i-th layer.  

\section{Hyperparameters of the 3D CNN System}
\begin{itemize}
    \item The number and the size of the filters. AlexNet has 5 filters: 11x11,5x5, 3x3,3x3,3x3. 

\item The size of the batch input. The batch size defines the number of samples that will be propagated through the network in every step. 

\item The learning rate. It controls how much to change the model in response to the estimated error each time the model weights are updated 

\item The number of the hidden layers. When considering the structure of dense layers, there are really two decisions that must be made regarding these hidden layers: how many hidden layers to actually have in the neural network and how many neurons will be in each of these layers. 

\item The number of the Fully Connected layers and their neurons. The same as for the filters. 

\item The type of the classifier. A classifier is an algorithm that maps the input data to a specific category. 

\item The type and the size of the pooling. Pooling, progressively reduces the size of feature maps. the different types of pooling operations are: Maximum Pool, Minimum Pool, Average Pool, Adaptive Pool. 
\end{itemize}

\chapter{3D CNN+LSTM-ConvLSTM}
 In the paper \cite{NOO2019} the proposed model model combines a 3D convolution neural network connected with a long short-term memory (LSTM). As input data there can be Depth data, RGB and multimodal data.The control with the Finite State Machine-FSM help to reduce some flows of the gesture and to border the classes of the recognition. Usage of small classes tends to showcase higher accuracy to that of big classes. The reuse of gestures for multiple commands in one application depends on the context of each application itself. The removing of the background and all the pixels that are unnecessary  is made by the attention on the hand. The global side of the hand gesture recognition is analyzed. The input data includes the whole handshape instead of the use of the finger feature for classification. (\ref{fig:fig22}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image37.jpg}
  \\
  \caption{The architecture of the model \cite{NOO2019}}
  \label{fig:fig22}
\end{figure}

\section{The Dataset of the System}
Twenty individuals’ data with five different environments and varying lighting conditions were collected as the dataset for training, validating and testing purposes. The datasets contains 2162 videos of participants while they perform 24 gestures, 13 static and 11 dynamic. Each gesture sequence contains a dynamic gesture of 3 seconds which consists of 120 frames. The sensor for the data was the depth camera Real Sense SR300 thus the dataset includes the RGB and Depth data. For the extraction of the hand, given the whole RGB image $I_r$, and depth image $I_d$, fixed distance dt to cut off the background and minimum distance min of $I_d$ as the range filter was defined. Let $I_{rb}$ be the RGB image and $I_db$ be Depth image, after the cut off of the background (\ref{fig:fig23}). The calculation of the average distance of a point $d_av$ in $I_db$ in the range [min, dt]:  $d_{av}=\frac {\sum_i^n I_{db}^i}{n}$ where $I_{db}^i>min$ and $I_{db}^i<d_t$

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image41.jpeg}
  \\
  \caption{The preprocess of the extraction of the based on the threshold  of the average depth \cite{NOO2019}}
  \label{fig:fig23}
\end{figure}

\section{The Multimodal Architecture}
The architecture of the model consists of 3D-CNN layers, followed by one stack LSTM layer and, then there is a fully connected layer and in the end the softmax layer. The use of the Batch normalization make possible for the model to utilize learning rates much higher. The size of the kernel of each Conv3D layer is 3 × 3 × 3, the stride and padding are sizes of 1 × 1 × 1. After the Conv3D layer, there is a batch normalization layer, followed by a ReLU layer and a 3D Max Pooling layer with a pool size of 3 × 3 × 3. There is an extraction of the features from the 3D-CNN, then there is an one stack of LSTM with 256 sizes of the unit. There is a value of 0.5 addition in every section in several dropout layers and then computed the output probability result using the softmax layer.

\subsection{Gates of LSTM}
\begin{itemize}
    \item Input gate: $i_t=\sigma(x_tw_{xi}+h_{(t-1)}w_{hi}+c_{(t-1)}w_{ci}+w_{ibais})$
    \item Forget gate: $f_t=\sigma(x_tw_{xf}+h_{(t-1)}w_{hf}+c_{(t-1)}w_{cf}+w_{fbais})$
    \item Output gate: $o_t=\sigma(x_tw_{xo}+h_{(t-1)}w_{ho}+c_{(t-1)}w_{co}+w_{obais})$
    \item Cell gate: $z_t=tahn(x_tw_{xz}+h_{(t-1)}w_{hz}+w_{zbais})$
    \item Memory gate: $c_t=z_t\bigotimes i_t+c_{(t-1)}\bigotimes f_t$
    \item Output activation (hidden state): $h_t=o_t\bigotimes tahn(c_t)$
    \item $c_t$ is the memory and $h_t$ is the output activation at a time $t$
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image48.png}
  \\
  \caption{The proposed 3DCNN + LSTM architecture \cite{NOO2019}}
  \label{fig:fig24}
\end{figure}

\section{Handling the Data}
The depth and RGB fused to form the input data, and that may have a better result rather than the use of input data from one stream. There are three kinds of multimodal types according to their fusion levels (\ref{fig:fig25}): 

\begin{itemize}
    \item Early fusion: 4 channels,3 channels RGB+1 channel depth, fused before the input to the 3D CNN layers. 
\item Middle fusion: the output of the separated 3D CNN layers, one from the RGB and the other from the Depth, are fused for the input to LSTM layer. 

\item Late fusion: in the same way, the fusion takes place after the LSTM layer and before Fully Connected layer. 
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image49.png}
  \\
  \caption{\cite{NOO2019}}
  \label{fig:fig25}
\end{figure}

\section{Context-Aware Neural Network}
For the increase of the rate of the recognition in the system there must be a recognition of smaller gesture class. In a Context-Aware recognition control system, the class for the recognition was bounded in every context. The Finite State Machine-FSM is attached with the Deep learning model and restrict the softmax decision probability with the manipulation of the weight in the last layer. The system in a current context or state interacts with the Finite State Machine to make a decision about which gesture not to be taken in account. The weights which were pre-defined to the last layer’s node that join to the FSM ignored class are applied, thus just the correct gestures are accepted. (\ref{fig:fig26}, \ref{fig:fig27}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image50.jpg}
  \\
  \caption{the FSM model controller with GRM (Gesture Recognition Machine) \cite{NOO2019}}
  \label{fig:fig26}
\end{figure}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image51.jpeg}
  \\
  \caption{A two contexts example  in FSM model \cite{NOO2019}}
  \label{fig:fig27}
\end{figure}

\chapter{Deep Learning Methods for Spatiotemporal Systems}
There are 4 ways of modelling for spatiotemporal systems \cite{EGO2018}:
\begin{itemize}
    \item The 2D ConvNets extract features of one frame. The classifiers are trained for the prediction of the labels of videos based on the frame features.

\item The 3D ConvNets can derive features of video clips. Afterwards, they accumulate the clip features into video descriptors. 

\item TThe  usage of recurrent neural networks-RNN to handle the temporal frames sequences is based on features of convolution. 

\item Formatting a video in one or in multiple compact frames and classify it with a neural network. 
\end{itemize}

\section{Evaluation of Models}
In paper \cite{EGO2018}, deep learning models are being compared using the dataset EgoGesture. The data splits into training set (60\%) 1.239 videos, in validation set (20\%) 411 videos and in testing set (20\%) sets 431 videos. To classify, there is a segmentation of the video sequences into isolated samples of gestures based on the first and the last frames, which have annotations in advance. The aim of the learning is to anticipate the labels of the class for each sample of gesture. The numbers of gesture samples in training, validation and testing splits are 14416, 4768 and 4977. For classification, the video sequences segmented into isolated gesture samples based on the beginning and ending frames annotated in advance. The learning task is the prediction of the class labels for each gesture sample. 

\section{The Deep Learning Models}
\textbf{VGG16} is a 2D CNN with 13 convolutional and 3 FC layers.
 \textbf{C3D} is a 3D CNN with eight 3D convolutional layers, one 2D pooling layers, four 3D pooling layers and three fully-connected layers. \textbf{C3D+hand mask}:C3D method is for the segmentation for the hand, since the depth camera get rid of most of the background information and consider the depth frame as a hand mask. \textbf{C3D+LSTM+RSTTM}: a C3D augmented model with a recurrent spatiotemporal transform module (RSTTM). \textbf{VGG16+LSTM} a single-layer LSTM with 256 hidden units after the first fully-connected layer of VGG16 .
 \textbf{IDMM+CaffeNet} handles spatial and temporal data of a video into an image called improved depth motion map (IDMM)and in this way there can be classified by 2D ConvNets.

\subsection{The Parameters of the Models}
The learning rate and the batch size were set as large as possible. When the loss is steady, the learning rate reduced with a fixed decay factor which is set to 10.

\begin{itemize}
    \item Stochastic Gradient Descent (SGD) is used for optimization. 

\item learning rate: VGG16 (0.001), C3D (0.003), VGG16+LSTM (0.0001). 

\item the step size of learning rate decay: VGG16 (5), C3D (5), VGG16+LSTM (10). 

\item Batch size: VGG16(60), C3D (20), VGG16+LSTM (20).  
\end{itemize}

In \ref{tab:tab1} the methods are compared based on the accuracy rates and the proposed model \textbf{C3D+LSTM+RSTTM} has the best rates. In \ref{tab:tab2}, the comparison is made with cross subject setting.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{METHOD}        & \textbf{RGB} & \textbf{DEPTH} & \textbf{RGB-D} \\
\hline
IDMM+CaffeNet          & -            & 0,664          & -              \\
\hline
VGG16 softmax          & 0,572        & 0,579          & 0,612          \\
\hline
VGG16 fc6              & 0,625        & 0,623          & 0,665          \\
\hline
VGG16+LSTM softmax     & 0,673        & 0,690          & 0,725          \\
\hline
VGG16+LSTM lstm7       & 0,747        & 0,777          & 0,814          \\
\hline
C3D fc6, 8 frames      & 0,817        & 0,844          & 0,865          \\
\hline
C3D softmax, 16 frames & 0,851        & 0,868          & 0,887          \\
\hline
C3D fc6, 16 frames     & 0,864        & 0,881          & 0,897          \\
\hline
C3D+HandMask           & -            & -              & 0,872          \\
\hline
C3D+LSTM+RSTTM         & 0,893        & 0,906          & 0,922          \\
\hline
\end{tabular}
\caption{Gesture Classification Accuracy of the models with EGOGESTURE Dataset \cite{EGO2018}}
\label{tab:tab1}
\end{table}

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Method}    & \textbf{Modality} & \textbf{Accuracy without CS} & \textbf{Accuracy with CS} & \textbf{Variance} \\ \hline
VGG16 fc6          & RGB               & 0,667                        & 0,625                     & 0,042             \\ \hline
VGG16+LSTM lstm7   & RGB               & 0,764                        & 0,689                     & 0,075             \\ \hline
C3D fc6, 16 frames & RGB               & 0,892                        & 0,864                     & 0,028             \\ \hline
VGG16 fc6          & depth             & 0,647                        & 0,623                     & 0,024             \\ \hline
VGG16+LSTM lstm7   & depth             & 0,801                        & 0,732                     & 0,069             \\ \hline
C3D fc6, 16 frames & depth             & 0,907                        & 0,881                     & 0,026             \\ \hline
VGG16 fc6          & RGB-D             & 0,697                        & 0,665                     & 0,32              \\ \hline
VGG16+LSTM lstm7   & RGB-D             & 0,826                        & 0,753                     & 0,73              \\ \hline
C3D fc6, 16 frames & RGB-D             & 0,922                        & 0,897                     & 0,025             \\ \hline
\end{tabular}
\caption{CS: cross subject setting, when the set of training and of testing  are from different participants. Classification accuracy with or without CS \cite{EGO2018}}
\label{tab:tab2}
\end{table}

\chapter{Comparison of Architectures}
Comparison of models which were trained with WLASL dataset \cite{DON2020}:
\begin{itemize}
    \item 2D CNN+RNN: RNN are used for the temporal relations and 2D CNN for the spatial features of the frames (\ref{fig:fig28}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image52.png}
  \\
  \caption{2D Conv. RNN \cite{DON2020}}
  \label{fig:fig28}
\end{figure}

\begin{itemize}
    \item 3D CNN can be applied for both the spatial and temporal relationship between the frames (\ref{fig:fig29}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image53.png}
  \\
  \caption{3D CNN \cite{DON2020}}
  \label{fig:fig29}
\end{figure}

\begin{itemize}
    \item Pose based models: use RNNs to interpret the sequences of the poses to analyze the movements. (\ref{fig:fig30}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image54.png}
  \\
  \caption{Pose RNN the keypoints are the joints of human bodies \cite{DON2020}}
  \label{fig:fig30}
\end{figure}

\begin{itemize}
    \item Temporal Graph Convolution Networks: (TGCN) models the spatiotemporal dependencies of the pose sequence (\ref{fig:fig31}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image55.png}
  \\
  \caption{Pose TGCN \cite{DON2020}}
  \label{fig:fig31}
\end{figure}

\section{Pose Based Temporal Graph NEURAL NETWORKS}
In the Temporal Graph Convolution Networks (TGCN), the input pose sequence: $X_{1:N}=
\begin{bmatrix}
	x_1, x_2, x_3 \dots x_N
\end{bmatrix}$, where N=frames and  \newcommand{\R}{\mathbb{R}} $X_i \in \R^K$ are the 2D keypoints in K dimensions. They encode the body movements as a holistic representation of the trajectories of body keypoints. In this way the dependencies among the joints of the human body are represented in a graph network. A residual graph convolutional block stacks two graph convolutional layers. A human body is represented as a fully-connected graph with K vertices and the edges in the graph as a weighted adjacency matrix: $A \in \R^{K \times K}$ In a deep graph convolutional network, the n-th graph layer is a function $G_n$ that take as input features a matrix: $H_n \in \R^{K \times F}$ . F is the feature dimension output by its previous layer. The set of trainable weights: $W_n \in \R^{F \times F'}$ . A graph convolutional layer is expressed as: $H_{n+1}=G_n(H_n)=\sigma(A_n H_n W_n)$ where $A_n$ is a trainable adjacency matrix for n-th layer and $\sigma$(·) denotes the activation function tanh(·) (\ref{fig:fig32}).

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image62.png}
  \\
  \caption{Residual Graph Convolution Block \cite{DON2020}}
  \label{fig:fig32}
\end{figure}

All the 4 systems are compared with different datasets (\ref{tab:tab3}). In particular, there is a selection of top-K glosses with K = {100, 300, 1000, 2000}, and they are organized to four subsets, named WLASL100, WLASL300, WLASL1000 and WLASL2000.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
{\textbf{Method}}    & {\textbf{WLASL 100}} & {\textbf{WLASL 300}} & {\textbf{WLASL 1000}} & { \textbf{WLASL 2000}} \\ \hline
{ \textbf{Pose GRU}}  & { 0,856}              & { 0,760}              & { 0,701}               & { 0,613}               \\ \hline
{ \textbf{Pose TGCN}} & { 0,876}              & { 0,796}              & { 0,719}               & { 0,622}               \\ \hline
{ \textbf{VGG+GRU}}   & { 0,639}              & { 0,610}              & { 0,493}               & { 0,325}               \\ \hline
{ \textbf{3D CNN}}    & { 0,899}              & { 0,869}              & { 0,843}               & { 0,663}               \\ \hline
\end{tabular}
\caption{Top-10 accuracy rates by each model on WLASL subsets with different number of glosses \cite{DON2020}.}
\label{tab:tab3}
\end{table}

The comparison became with combinations of the training and the testing sets (\ref{tab:tab4}). 

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
{ \textbf{\begin{tabular}[c]{@{}c@{}}TRAINING \\ SET/ \\ TESTING \\ SET\end{tabular}}} & \multicolumn{2}{c|}{{ \textbf{WLASL100}}}                 & \multicolumn{2}{c|}{{ \textbf{WLASL300}}}                 & \multicolumn{2}{c|}{\textbf{WLASL1000}} & \multicolumn{2}{c|}{\textbf{WLASL2000}} \\ \hline
{ \textbf{}}                                                                       & { \textbf{3D CNN}} & { \textbf{TGCN}} & { \textbf{3D CNN}} & { \textbf{TGCN}} & \textbf{3D CNN}     & \textbf{TGCN}     & \textbf{3D CNN}     & \textbf{TGCN}     \\ \hline
{ \textbf{WLASL100}}                                                               & { 0,899}           & { 0,876}         & { -}               & { -}             & -                   & -                 & -                   & -                 \\ \hline
{ \textbf{WLASL300}}                                                               & { 0,883}           & { 0,814}         & { 0,869}           & { 0,796}         & -                   & -                 & -                   & -                 \\ \hline
{ \textbf{WLASL1000}}                                                              & { 0,852}           & { 0,775}         & { 0,862}           & { 0,742}         & 0,843               & 0,719             & -                   & -                 \\ \hline
WLASL2000                                                                                              & 0,720                                  & 0,678                                & 0,711                                  & 0,654                                & 0,673               & 0,645             & 0,663               & 0,622             \\ \hline
\end{tabular}
\caption{Top-10 accuracy rates of 3D CNN and Pose-TGCN with different training set (rows) and testing set (columns) on WLASL subsets \cite{DON2020}}
\label{tab:tab4}
\end{table}

\chapter{Skeleton-Based Gesture Recognition}
The paper \cite{LIU2020} proposed an architecture which is built for a skeleton-based Gesture recognition and its approach is that the gesture is a sequence of complexly composite movements. The innovation of this architecture is that it is combined of two model: one applied on the hand posture variations and the other on the hand movements. The method can simultaneously enhance the expressive power of the posture and motion information. \textbf{HPEV} (3D hand posture evolution volume) is the model applied on the posture variations and \textbf{HMM} (2D hand movement map) model captures holistic movements. The HPEV integrates spatiotemporal information of hand postures with a 3D CNN, and on the other hand, the HMM uses a 2D CNN model to manipulate hand movement features. A unified two-stream framework learn the decoupled representations of the gestures (\ref{fig:fig33}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image63.png}
  \\
  \caption{Hand gesture separated into variations of hand posture and hand movements \cite{LIU2020}}
  \label{fig:fig33}
\end{figure}

\section{FRPV - Fingertip Relative Position Vector}
FRPV describes the movement of the finger and the positions of the 4 fingers and the thumb of each frame construct it. For frame t, the four relative positions are concatenated as a vector $u_t$ as follows: $$u_t=(p_{I,t},p_{M,t},p_{R,t},p_{L,t})-(p_{0,t},p_{0,t},p_{0,t},p_{0,t})$$
where p (0, t) is the coordinate of thumb of the t-th frame, and \textit{p(I, t), p(M, t), p(R, t) and p(L, t)} are the coordinates of index fingertip, middle fingertip, ring fingertip and little fingertip at frame t respectively. The FRPV is the concatenation of all the $u_t$ of each frame [N= number of frames]: $U_{FRPV}=(u_1,u_2,\dots,u_t,\dots,u_N)$

\section{Architecture of the System}
The system consists two main streams and each of them extracts a vector of the features that has built for: HPEV-Net and HMM-Net. HPEV-Net is applied for the hand movement map and uses a 3D CNN for the low-level features with the size of the kernel 7x3x3 and afterwards there is a stack of four bottleneck modules \cite{KAI2016} for the high-level features. In every CNN layer the activation function is RELU, and the batch normalization is used. Also, the max pooling layers are 4x2x2. The output channels of the four bottleneck modules are 128, 128, 256 and 512. The output of the last bottleneck after global average pooling ends into a feature vector. The Fingertip Relative Position Vector (FRPV), is applied to describe movements of the fingers, because the restrictions on the resolution make very difficult the encoding of these movements. The output of FRPV goes to the next FC layer with Batch Normalization and ReLU activation. In the same stream, the outputs of HPEV and FRPV, are concatenated and there is a classification of the hand gesture sequences with a softmax algorithm. On the other stream, Hand Movements Map (HMM), uses a 2D CNN for the for the motion of the hand. HMM-Net is based on the Hierarchical Co-occurrence Network (HCN) the paper's \cite{CHA2018} proposed network, to extract features. In the same way like the HPEV-Net, there is a stack of four bottleneck modules. The output channels of the four bottleneck modules are 128, 128, 256 and 512. The output of the last bottleneck after global average pooling ends into a feature vector (\ref{fig:fig34}). 

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image66.png}
  \\
  \caption{The two Neural Networks HPEV-Net+HMM-Net \cite{LIU2020}}
  \label{fig:fig34}
\end{figure}

\section{Datasets for Analysis}
\begin{itemize}
    \item SHREC’17 Track: Dataset of the paper \cite{QUE2017} contains 14 gestures. They are performed twice: with one finger and with the whole hand. It includes 2800 sequences, 1960 for the training set and 840 for testing set. 

    \item DHG-14/28: The dataset of the paper \cite{HAZ2016} comprises 14 gestures with 2800 sequences. The DHG-14/28 and the SHREC’17 Track datasets have the same hand joints and the same method of data collection.  

    \item FPHA: The last dataset in the paper \cite{GUI2018} provides dynamic hand sequences. It includes 1175 action had sequences, with 45 categories handling 26 different objects in 3 scenarios. It has one less hand joint from the SHREC’17 Track dataset. The training set (600 sequences) and testing set (575 sequences) have almost the same percentage of data (\ref{fig:fig35}).
\end{itemize}

\begin{figure}[!htbp]
\centering
 % Requires \usepackage{graphicx}
  \includegraphics[width=0.55\textwidth]{figures/image67.png}
  \\
  \caption{Gesture recognition accuracy rates with the use of  different combinations of input on FPHA dataset \cite{LIU2020}}
  \label{fig:fig35}
\end{figure}

A comparison of the separated networks that form the system \ref{tab:tab5}

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
{ }                                  & \multicolumn{2}{c|}{{ \textbf{SHREK}}}                & { }                                \\ \cline{2-3}
\multirow{-2}{*}{{ \textbf{Method}}} & { \textbf{14G}} & { \textbf{28G}} & \multirow{-2}{*}{{ \textbf{FPHA}}} \\ \hline
{HPEV}                     & { 0,734}        & { 0,714}        & { 0,770}                           \\ \hline
{HMM}                      & { 0,927}        & { 0,866}        & { 0,677}                           \\ \hline
FRPV                                                     & 0,628                               & 0,588                               & 0,664                                                  \\ \hline
HPEV+HMM                                                 & 0,944                               & 0,902                               & 0,829                                                  \\ \hline
HPEV+HMM+FRPV                                            & 0,948                               & 0,922                               & 0,909                                                  \\ \hline
\end{tabular}
\caption{Gesture Recognition accuracy rates for different input combinations on SHREC’17 Track and FPHA dataset.14G:14 gestures, 28G:28 gestures \cite{LIU2020}.}
\label{tab:tab5}
\end{table}

In table 6 (\ref{tab:tab6}), there is a comparison of some proposed models performing with the SHREC’17 dataset. The paper \cite{LIU2020} proposed model has the best accuracy rate.

\begin{table}[!htbp]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{METHOD}                       & \textbf{ACCURACY 14G} & \textbf{ACCURACY 28G} \\ \hline
HON4D \cite{OMA2013}                  & 0,785                 & 0,740                 \\ \hline
SoCJ+Direction+Rotation \cite{SME2017} & 0,869                 & 0,842                 \\ \hline
SoCJ+HoHD+HoWR \cite{HAZ2016}          & 0,882                 & 0,819                 \\ \hline
Two-stream 3D CNN \cite{JUA2018}       & 0,834                 & 0,774                 \\ \hline
Res-TCN \cite{JIN2018}                 & 0,911                 & 0,873                 \\ \hline
STA-Res-TCN \cite{JIN2018}            & 0,936                 & 0,907                 \\ \hline
ST-GCN \cite{YAN2018}                  & 0,927                 & 0,877                 \\ \hline
ST-TS-HGR-NET \cite{XUA2019}           & 0,942                 & 0,894                 \\ \hline
DG-STA \cite{YUX2019}                  & 0,944                 & 0,907                 \\ \hline
HPEV+HMM+FRPV                         & 0,948                 & 0,922                 \\ \hline
\end{tabular}
\caption{Gesture recognition comparison of some of the latest proposed models with SHREC’17 dataset 14G:14 gestures, 28G:28 gestures \cite{LIU2020}}
\label{tab:tab6}
\end{table}

\bibliographystyle{abstract}
\bibliography{refs.bib}

\end{document} 